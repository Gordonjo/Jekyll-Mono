[{"authors":["admin"],"categories":null,"content":"I am a Ph.D. candidate with the Computational and Biological Learning group at the University of Cambridge, supervised by Dr José Miguel Hernández-Lobato and advised by Dr Richard Turner. My research focuses on developing probabilistic models (typically parameterized by deep neural networks) and associated scalable approximate inference procedures.\n","date":1578614400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1578614400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a Ph.D. candidate with the Computational and Biological Learning group at the University of Cambridge, supervised by Dr José Miguel Hernández-Lobato and advised by Dr Richard Turner. My research focuses on developing probabilistic models (typically parameterized by deep neural networks) and associated scalable approximate inference procedures.","tags":null,"title":"Jonathan Gordon","type":"authors"},{"authors":["Jonathan Gordon*","Wessel Bruinsma*","Andrew Y. K. Foong","James Requeima","Yann Dubois","Richard Turner"],"categories":null,"content":"","date":1578614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578614400,"objectID":"df9dd04a0747013673a081e10656b6f7","permalink":"/publication/convcnp/","publishdate":"2020-01-10T00:00:00Z","relpermalink":"/publication/convcnp/","section":"publication","summary":"We extend deep sets to functional embeddings and Neural Processes to include translation equivariant members (Oral Presentaiton).","tags":["Neural processes","Meta-learning","Translation equivariance","Deep sets","Deep Learning"],"title":"Convolutional Conditional Neural Processes","type":"publication"},{"authors":["Jonathan Gordon","David Lopez-Paz","Marco Baroni","Diane Bouchecourt"],"categories":null,"content":"","date":1578614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578614400,"objectID":"a9d20fb1f76c818ccf3190eae5d9c94a","permalink":"/publication/compositional/","publishdate":"2020-01-10T00:00:00Z","relpermalink":"/publication/compositional/","section":"publication","summary":"We propose a link between permutation equivariance and compositional generalization, and provide equivariant language models","tags":["Natural language","Permutation equivariance","Deep learning"],"title":"Permutation Equivariant Models for Compositional Generalization in Language","type":"publication"},{"authors":["James Requeima*","Jonathan Gordon*","John Bronskill*","Sebastian Nowozin","Richard Turner"],"categories":null,"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"9f89dfbdcb1767763b5fbc6548afd1ce","permalink":"/publication/cnaps/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/publication/cnaps/","section":"publication","summary":"Powerful meta-learning system based on the neural process framework (Spotlight)","tags":["Meta learning","Few-shot learning","Neural processes","Deep learning","Approximate inference"],"title":"Fast and Flexible Multi-Task Classification Using Conditional Neural Adaptive Processes","type":"publication"},{"authors":["Robert Pinsler","Jonathan Gordon","Eric Nalisnick","Jose Miguel Hernandez-Lobato"],"categories":null,"content":"","date":1568073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568073600,"objectID":"7ed5a673bf7e105d486ca4fcafd53f70","permalink":"/publication/batch-al/","publishdate":"2019-09-10T00:00:00Z","relpermalink":"/publication/batch-al/","section":"publication","summary":"We reconsider the active learning problem, leveraging advances in Bayesian coresets to relieve the standard myopic assumption.","tags":["Active learning","Approximate inference"],"title":"Bayesian Batch Active Learning as Sparse Subset Approximation","type":"publication"},{"authors":["Jonathan Gordon","Jose Miguel Hernandez-Lobato"],"categories":null,"content":"","date":1557446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557446400,"objectID":"6a0dc3ccbba9b83a74ae2f238cda8adc","permalink":"/publication/combining/","publishdate":"2019-05-10T00:00:00Z","relpermalink":"/publication/combining/","section":"publication","summary":"We introduce a framework for combining deep generative and discriminative models for semi-supervised learning.","tags":["Semi-Supervised learning","Approximate inference","Deep Learning","Generative models"],"title":"Combining Deep Generative and Discriminative Models for Bayesian Semi-Supervised Learning","type":"publication"},{"authors":["Marton Havasi","Jasper Snoek","Dustin Tran","Jonathan Gordon","Jose Miguel Hernandez-Lobato"],"categories":null,"content":"","date":1544400000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544400000,"objectID":"9a8f2cb731c219b8a16535c6856ea053","permalink":"/publication/refining/","publishdate":"2018-12-10T00:00:00Z","relpermalink":"/publication/refining/","section":"publication","summary":"Iteratively improving variational posteriors for BNNs with gradient descent and auxiliary variables.","tags":["Approximate inference","Variational Inference","Bayesian Neural Networks"],"title":"Refining the Variational Posterior Through Iterative Optimization","type":"publication"},{"authors":["Jonathan Gordon*","John Bronskill*","Matthias Bauer","Sebastian Nowozin","Richard Turner"],"categories":null,"content":"","date":1523318400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523318400,"objectID":"6eea35a8ecff00f74cce4c04fa0453c5","permalink":"/publication/mlpip/","publishdate":"2018-04-10T00:00:00Z","relpermalink":"/publication/mlpip/","section":"publication","summary":"We introduce ML-PIP, a general probabilistic framework for meta-learning","tags":["Meta-learning","Few-shot Learning","Deep Learning"],"title":"Meta Learning Probabilistic Inference for Prediction","type":"publication"},{"authors":null,"categories":null,"content":"An interesting debate has arisen lately in the machine learning community concerning two competing (?) approaches to ML and (more generally) AI. The debate is rather high-level, but in my opinion touches upon something that is at the very core of research in the field. In this post I will lay out the fundamental aspects of the debate as I see them, and try and give my personal perspective on the issue.\nConnectionist, Gradient Based Learning  Undoubtably the most dominant trend in ML at the moment is deep learning - i.e., learning that is based on neural networks and their offspring achieved by applying gradient-based methods (error-back propagation, a.k.a backprop). DL has led to some undeniably outstanding successes, achieving human-level (or better) performance in specific tasks such as object recognition, speech (recognition and synthesis), and game-playing. This in turn has led to wide-spread adoption and integration of DL technologies in many leading tech companies, and has generated a lot of media hype and public recognition.\nDespite these impressive successes, there are major drawbacks to DL. The obvious problems are terrible data-efficiency and an inability to generalize across tasks or domains. Both of these characteristics arise from the fact that neural networks are at their core pattern matching machines. Essentially, they find complex patterns in massive datasets that correlate with a desired output. Often, these patterns may correspond to interesting notions of underlying structure in the data (i.e., edges in an image), lending to the notion of representation learning. However, these representations are highly specified for specific tasks, and training a network for a new task must be done from scratch. This can be frustrating since we often intuitively know there must be transferable knowledge between the tasks. An example of this is the phenomenon known as catastrophic forgetting [1], where performance of a trained network on task A will deteriorate significantly when trained for related task B. Essentially, every time we train a neural network we must do so (pretty much) from scratch, even if there are many shared components that should be leveraged.\nA Real-World Counter Example  Given the recent successes of DL and gradient-based learning, and the impressive ability of generic neural networks to learn meaningful representations, should we in fact consider alternative approaches? Well, we have a biological counter-example of general intelligence that works - humans. As many have pointed out, artificial flight was achieved when humans moved away from biological inspirations. This is a valid point, and I do not believe we should limit our investigation of intelligent systems to mimicking human intelligence. On the other hand, human intelligence provides an excellent benchmark for measuring performance of artificial systems and a source from which to draw aspirations.\nIn context of this post, we observe that humans learn rich representations that generalize across complex tasks from very few examples. For instance, given a single visual example of a new object, humans easily infer high level traits such as its purpose, decomposition into parts and the relations between them, and how it may interact with different environments (image and motivation taken from [2]). They can then use these traits to classify new visual examples of the object, draw (or imagine) variations on the object, or think of uses for it. All of this from observing a single example.\n{:refdef: style=\u0026quot;text-align: center;\u0026quot;} {:refdef}\nThe presence of these traits in human intelligence serves both to highlight the drawbacks of neural-network based learning and perhaps indicate that DL alone will not be sufficient to achieve significant progress towards more general intelligence. Further, (though I am no neuroscientist), there seems to be sufficient evidence it is highly unlikely the brain is composed of sets of generic, single purpose neural networks [3].\nIn [3], the authors argue that the ability of humans to learn such complex representations of new concepts given little data is due to the existence of models that allow humans to leverage their past experiences to do so. They go on to argue that important characteristics of these models are the presence of fundamental learning structures from early age (e.g., intuitive physical and psychological knowledge), causality, and compositionality. Importantly, none of these are characteristics of DL.\nThe Heart of the Debate Given what is discussed above, we can now ask the central question, which is exemplified in two recent papers [3, 4]: how much emphasis should we place on encoding knowledge into our models?\nThe Case for Model-Free ML  Among the main proponents of learning from scratch and largely model free ML are researchers at Google DeepMind, who authored [4]. DeepMind are responsible for some of the most noteworthy successes of DL such as mastering Atari games [5] and Go [6]. These successes have been achieved with advances in model-free systems. Indeed, the only inductive biases introduced in these systems is that of standard convolutional nets for parsing images. As discussed in [4], there are (at least) two significant arguments against incoporating and encoding prior knowledge into our models.\nThe first has to do with the notion of generality (not generalization). For many domains, prior or expert knowledge may not be available, or may be intractable to encode. For instance, physical laws can provide important prior knowledge in domains such as robotic movement, and the model-based approach justifies leveraging that knowledge. This may indeed improve performance of deployed systems. However, in many domains (e.g., healthcare, dialogue systems) such knowledge may not be so straight-forward to include. How would a general model-based approach towards intelligence then deal with these domains? To quote [4]:\n\u0026ldquo;\u0026hellip; it is not clear that detailed knowledge engineering will be realistically attainable in all areas we will want our agents to tackle\u0026quot;.\nWith this in mind, generic models that make no prior assumptions on the domain and \u0026ldquo;learn from scratch\u0026rdquo; may be more generally applicable to a wider range of areas.\nThe second point has to do with avoiding encoding our own biases into intelligent agents. The knowledge and inductive biases that [3] argue should be included in our intelligent models are specifically human. There is no reason to believe that these principles are required (let alone optimal) for machines to be intelligent. Indeed, encoding incomplete notions of inductive biases in an attempt to mimick the human brain may hinder progress, much as attempting to achieve artificial flight by mimicking birds proved unproductive. Proponents of model-free learning argue that by starting from a \u0026lsquo;blank slate\u0026rsquo;, machines generate representations and inductive biases that are useful to them for the specified task. From this view, learning from scratch is a feature rather than a bug.\nProponents of the model free approach argue that in fact we should avoid encoding knowledge into our models for the reasons specified above. Existing problems in neural networks, such as data efficiency and transferability, can be solved within the context of DL and do not require prior/expert knowledge. Recent advances in DL (such as memory or attention mechanisms, deep generative models) are indeed steps in these directions, though it is not clear that these will solve the core issues.\nHow Much is Really Gained by Models?  The main motivation for heavily engineered models is achieving human-like learning: rich, generalizable representations of complex concepts with little examples. However, is it clear that encoding knowledge into models can actually achieve this? Below I discuss an example that shows how heavily engineered models can indeed get around some of the major difficulties of DL.\nGame-Playing with Minor Environmental Variations In [5], the authors show that a single deep RL algorithm can achieve human-level performance on a wide range of Atari games using only the screen pixels as input. This is a very impressive achievement, and one that has been subsequently improved upon. However, in [7] the authors point out a major flaw of those deep RL agents: minor variations in the environment completely break performance.\n{:refdef: style=\u0026quot;text-align: center;\u0026quot;} {:refdef}\nIn other words, the \u0026ldquo;knowledge\u0026rdquo; gained by the agent while learning to play the original version of the game is so specific that any variation to the environment (including ones that humans easily adapt to) is a completely new domain and the agent must be retrained. Clearly, this is unsatisfying, and we desire our agents to be more general than this.\nThe authors go on to propose schema networks; complex graphical models that model objects, their movements and attributes, and causal relations between actions/objects/reward. This allows the agent to perform long term planning (phrased as inference) after training. The paper then details experiments showing that schema networks easily adapt to the variations in environment that break the deep RL agents.\nBeside causal relations, the schema network model encodes knowledge on intuitive physics, namely that objects are smooth, and contain physical attributes relating to movement, size, etc. This paper demonstrates how encoding fundamental principles of causality and physics can enable models to be more robust to changes (as well as learn more efficiently).\nFor interested readers, other excellent examples of how models improve both data-efficiency and generalization can be found in [2, 8].\nMy Own Two Cents  Personally, I find this debate fascinating, and at the very core of what AI and ML research are all about. Both sides make compelling points, and it is not clear to me that one is correct. My natural inclination leans more towards the modeling perspective: I believe that intelligence has much to do with (probabilistic) models, and whether or not these draw inspitation from human intelligence, I cannot imagine an intelligent agent that is completely model free.\nHowever, I see no reason for the two approaches to be mutually exclusive (this notion is also emphasized in [3]). Much of the most interesting recent work has been on teaching neural networks to perform probabilistic inference. This is where I believe the most interesting and promising work currently lies: bridging the gap between DL and model-based ML. Neural networks are incredibly powerful tools, and integrating them into the toolbox of probabilistic modelling holds enormous potential.\nFurther, a potential goal is to merge model-based and model-free ML. My opinion is that, where possible, we should encode (fundamental) notions such as physics and theory of mind into our models, and neural networks can provide flexible mappings for complex relations. Where no such expert knowledge exists, latent variable models with neural network parameterizations can provide a powerful avenue to allow systems to learn abstract concepts from scratch. Ideally, these notions can co-exist within single systems.\nAs in many debates, my hunch is that the most progress can be made by integrating both sides, trying to take the best of both worlds.\nReferences    Goodfellow J., Ian, et al. An Emprical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks. 2013 \u0026#x21a9;\u0026#xfe0e;\n Lake M., Brenden, Salakhutdinov, Ruslan, and Tenenbaum B. Joshua. Human-level Concept Learning Through Probabilistic Program Induction. 2015 \u0026#x21a9;\u0026#xfe0e;\n Lake M., Brenden, et al. Building Machines that Learn and Think Like People. 2017 \u0026#x21a9;\u0026#xfe0e;\n Botvinick, Matthew, et al. Building Machines that Learn and Think for Themselves: Commentary on Lake, Ullman, Tenebaum, and Gershamn. 2017 \u0026#x21a9;\u0026#xfe0e;\n Mnih, Volodymyr, et al. Human-Level Control Through Deep Reinforcement Learning. 2015 \u0026#x21a9;\u0026#xfe0e;\n Silver, David, et al. Mastering the Game of Go Without Human Knowledge. 2017 \u0026#x21a9;\u0026#xfe0e;\n Kansky, Ken, et al. Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics. 2017 \u0026#x21a9;\u0026#xfe0e;\n George, Dileep, et al. A Generative Vision Model that Trains with High Data-Efficiency and Breaks Text-Based CAPTCHAs. 2017 \u0026#x21a9;\u0026#xfe0e;\n   ","date":1514851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514851200,"objectID":"cb64072eab2cfa81ef3d83847ee48691","permalink":"/post/model-debate/","publishdate":"2018-01-02T00:00:00Z","relpermalink":"/post/model-debate/","section":"post","summary":"An interesting debate has arisen lately in the machine learning community concerning two competing (?) approaches to ML and (more generally) AI. The debate is rather high-level, but in my opinion touches upon something that is at the very core of research in the field. In this post I will lay out the fundamental aspects of the debate as I see them, and try and give my personal perspective on the issue.","tags":null,"title":"On Model-Based vs. Model-Free AI","type":"post"},{"authors":null,"categories":null,"content":"Few-shot learning is (in my opinion) one of the most interesting and important research areas in ML. It touches at the very core of what ML/DL can’t do today, and is one of the clear indicators of how much work is left. All the approaches to few-shot learning I am aware of use probabilistic modelling. In fact, one of the reasons I’m so sold on Bayesian learning is that it offers avenues to pursue these problems - its not clear to me how one can even approach this not from a probabilistic perspective.\nA Discriminative Approach  An interesting paper by Rich Turner's group (in collaboration with Bernard Scholkopf's group) [1] proposed using the representations found in the final hidden layer of a deepNet to generalize to unseen classes for classification.\n{:refdef: style=\u0026quot;text-align: center;\u0026quot;} {:refdef}\nBy placing a prior on the weights they can perform posterior inference on the values for a new class, and show impressive results using very simple priors and likelihood functions. This is the only discriminative approach I am familiar with, and I think it shows a lot of promise for future work.\nGenerative Models for Few-Shot Learning  More common are the use of generative models. These can largely be placed on a spectrum of how structured the models are.\nUnstructured Models On one end of the generative spectrum are completely unstructured models [2, 3]. Here, researchers attempt to learn meaningful representations in latent variables, and then make use of that ‘information’ for new tasks. This has shown good results, even though optimal mechanisms for information sharing are not yet clear. A lot of interesting work is being done here.\nHeavily Structured Models On the other hand are heavily structured models [4, 5, 6]. These works come from a faction of ML researchers who believe we should encode as much knowledge as possible into our models [7]. For example, in [5] the authors show how a heavily engineered model can transfer its capabilities to unseen scenarios in game playing, something that state of the art deep RL agents are catastrophically bad at.\n{:refdef: style=\u0026quot;text-align: center;\u0026quot;} {:refdef}\nThere is a tradeoff here: structured models achieve very impressive generalization, sample efficiency, and few-shot capabilities. It is clear that by encoding knowledge we can achieve significant gains in these aspects. On the other hand, this entails expert knowledge and the ability to encode it in a computationally tractable manner. This is obviously not possible for all domains, and it may not be a scalable approach to improving AI [8]. Conversely, unstructured generative modelling relieves the burden of human engineering, which is a clear goal of ML. However, it is not clear how to guide the latent variables towards useful representations, nor is it clear what is the ‘right’ way to then transfer this knowledge.\nOne interesting recent paper [9] proposes a method for encoding known axes of variation in partially observed variables while allowing latent variables to capture unknown axes of variation. The paper proposes a general approach for variational inference in these models. I believe this approach of finding the middle ground of the spectrum will be very influential in the near future.\nOriginally a Quora answer\nReferences    Bauer, Matthias, et al. Discriminative K-Shot Learning Using Probabilistic Models. 2017 \u0026#x21a9;\u0026#xfe0e;\n Rezende J., Danilo, et al. One-Shot Generalization in Deep Generative Models. 2016 \u0026#x21a9;\u0026#xfe0e;\n Edwards, Harrison, and Storkey, Amos. Towards a Neural Statistician. 2017 \u0026#x21a9;\u0026#xfe0e;\n Salakhutdinov, Ruslan, Tenenbaum, Joshua, Torralba, Antonio. One-Shot Learning with a Hierarchical Nonparametric Bayesian Model. 2012 \u0026#x21a9;\u0026#xfe0e;\n Kansky, Ken, et al. Schema Networks: Zero-Shot Transfer with a Generative Causal Model of Intuitive Physics. 2017 \u0026#x21a9;\u0026#xfe0e;\n George, Dileep, et al. A Generative Vision Model that Trains with High Data-Efficiency and Breaks Text-Based CAPTCHAs. 2017 \u0026#x21a9;\u0026#xfe0e;\n Lake M., Brenden, et al. Building Machines that Learn and Think Like People. 2017 \u0026#x21a9;\u0026#xfe0e;\n Botvinick, Matthew, et al. Building Machines that Learn and Think for Themselves: Commentary on Lake, Ullman, Tenebaum, and Gershamn. 2017 \u0026#x21a9;\u0026#xfe0e;\n Siddarth, N., et al. Learning Disentangled Representations with Semi-Supervised Deep Generative Models. 2017 \u0026#x21a9;\u0026#xfe0e;\n   ","date":1514505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514505600,"objectID":"f8fc7838d4071bda0c564db8461010fb","permalink":"/post/few-shot-learning/","publishdate":"2017-12-29T00:00:00Z","relpermalink":"/post/few-shot-learning/","section":"post","summary":"Few-shot learning is (in my opinion) one of the most interesting and important research areas in ML. It touches at the very core of what ML/DL can’t do today, and is one of the clear indicators of how much work is left. All the approaches to few-shot learning I am aware of use probabilistic modelling. In fact, one of the reasons I’m so sold on Bayesian learning is that it offers avenues to pursue these problems - its not clear to me how one can even approach this not from a probabilistic perspective.","tags":null,"title":"Recent Advances in Few-Shot Learning","type":"post"},{"authors":["Francesco Paolo Casale","Jonathan Gordon","Nicolo Fusi"],"categories":null,"content":"","date":1512864000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512864000,"objectID":"a621abb1a6d173dcdd799c7dd72a7e14","permalink":"/publication/nas/","publishdate":"2017-12-10T00:00:00Z","relpermalink":"/publication/nas/","section":"publication","summary":"A probabilistic and differentiable framework for neural architecture search that improves speed and memory efficiency.","tags":["Architecture search","Approximate inference","Deep Learning"],"title":"Probabilistic Neural Architecture Search","type":"publication"},{"authors":null,"categories":null,"content":"A Bayesian neural network (BNN) refers to extending standard networks with posterior inference. They arise naturally when priors are placed on the weights of a network. They are relatively simple beasts, and (in my opinion) far more interesting than the model itself is the approximate posterior inference associated with them.\n Standard NN training via optimization is (from a probabilistic perspective) equivalent to maximum likelihood estimation (MLE) for the weights. For many reasons this is unsatisfactory. One reason is that it lacks proper theoretical justification from a probabilistic perspective: why maximum likelihood? Why just point estimates? Using MLE ignores any uncertainty that we may have in the proper weight values. From a practical standpoint, this type of training is often susceptible to overfitting, as NNs often do.\nOne partial fix for this is to introduce regularization. From a Bayesian perspective, this is equivalent to inducing priors on the weights (say Gaussian distributions if we are using L2 regularization). Optimization in this case is akin to searching for MAP estimators rather than MLE. Again from a probabilistic perspective, this is not the right thing to do, though it certainly works well in practice.\nThe correct (i.e., theoretically justifiable) thing to do is posterior inference [1, 2], though this is very challenging both from a modelling and computational point of view. BNNs are neural networks that take this approach. In the past this was all but impossible, and we had to resort to poor approximations such as Laplace’s method (low complexity) or MCMC (long convergence, difficult to diagnose). However, lately there have been some super-interesting results on using variational inference to do this [3], and this has sparked a great deal of interest in the area.\nBNNs are important in specific settings, especially when we care about uncertainty very much. Some examples of these cases are decision making systems, (relatively) smaller data settings, Bayesian Optimization, model-based reinforcement learning and others.\nOriginally a Quora answer, later a KDnuggets post\nReferences    Radford, Neal. Bayesian Learning for Neural Networks. 2012 \u0026#x21a9;\u0026#xfe0e;\n Mackay, David. Bayesian Neural Networks and Density Networks. 1995 \u0026#x21a9;\u0026#xfe0e;\n Blundell, Charles, et al. Weight Uncertainty in Neural Networks. 2015 \u0026#x21a9;\u0026#xfe0e;\n   ","date":1504569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504569600,"objectID":"c5bfdf5f6822e8c39a1535638512b6e2","permalink":"/post/bayesian_nnets/","publishdate":"2017-09-05T00:00:00Z","relpermalink":"/post/bayesian_nnets/","section":"post","summary":"A Bayesian neural network (BNN) refers to extending standard networks with posterior inference. They arise naturally when priors are placed on the weights of a network. They are relatively simple beasts, and (in my opinion) far more interesting than the model itself is the approximate posterior inference associated with them.\n Standard NN training via optimization is (from a probabilistic perspective) equivalent to maximum likelihood estimation (MLE) for the weights. For many reasons this is unsatisfactory.","tags":null,"title":"What is a Bayesian Neural Network?","type":"post"},{"authors":null,"categories":null,"content":"Recently I've had to train a few deep generative models with stochastic backpropagation. I've been working with variational autoencoders and Bayesian neural networks. If you've read the literature on these training procedures and models, you probably found the theory quite complete. When I implemented these models however, I found that quite a lot of elbow grease is required to get them to work well.\nFrom talking to some other researchers here, it seems like people dealing with these models are thirsty for practical advice. In this post, I will provide some background on the subject, and highlight some tips and hacks that really helped me with successfull implementation.\nDeep Generative Models  We are considering the case where we have some complex latent variable model (later we will see how we can consider the weights as our latent variables and extend these ideas to BNNs) as shown below.\n{:refdef: style=\u0026quot;text-align: center;\u0026quot;} {:refdef}\nHere, \\(x\\) are our inputs, \\(z\\) are the latent variables, and \\(\\theta\\) parameterizes the conditional distribution. Usually, we use deep neural networks to map from \\(z\\) to \\(x\\), so \\(\\theta\\) will be the parameters of the network. This is a very powerful family of models known as deep generative models (DGMs), even for very simple distributions of \\(z\\). What we would like to do is perform learning (i.e., maximum likelihood or a-posteriori estimation) for \\(\\theta\\), and inference for \\(z\\). Unfortunately, the posterior distribution for \\(z\\) is intractable, so something like EM would not work for us here.\nVariational Inference for DGMs  The best approach seems to be variational inference (VI). The basic idea with VI is to introduce an approximation to the true posterior, which we`ll call \\(q\\), and parameterize it with the variational parameters - \\(\\phi\\). To do this, we need to first choose some parameteric family for \\(q\\), such as a Gaussian. Having chosen \\(q\\), the idea is to minimize the distance between the true posterior and our approximation. The minimization is over \\(\\phi\\) and with respect to some divergence between distributions, such as the KL-divergence. Once we've optimized \\(q\\), we can use it instead of \\(p\\) whenever we need the posterior distribution. The variational objective can be expressed as:\n\\begin{equation} \\mathcal{L}(\\theta, \\phi; x) = \\mathbb{E}[\\log p(x|z)] - KL(q(z|x)||p(z)) \\end{equation}\nwhich has a nice interpretation: the first term can be seen as reconstruction error - we are trying to maximize the likelihood of the data under the latent variable. The second term can be interpreted as a regularizer - it encourages the approximation to be as simple as possible by penalizing it for being different from the prior on \\(z\\), which is typically chosen to be something simple like a standard normal distribution.\nWhat's fantastic about VI is that it allows us to convert inference from an integration problem (which we pretty much suck at) to an optimization one (which we are awesome at). On the flip-side, there are a few drawbacks: one problem is that we are usually limited to very simple posterior approximations, and the quality of our trained model is directly related to the quality of \\(q\\). Another problem is that posterior inference includes a separate set of variational parameters \\(\\phi\\) for every data-point, and therefore needs to be recomputed for every new example we receive.\nInference Networks and Reparameterization  To get around these problems, some guys had the brilliant idea of introducing an inference network. The idea is to use a neural network to learn \\(q\\). Ideally, we would train both networks \\(\\theta, \\phi\\) jointly. The main advantage of this is that it ammortizes posterior inference, so that while the number of latent variables grows linearly with the number of data points, posterior inference now has fixed computational and statistical complexity. This seems great, but of course, there are still some problems.\nTo train the things, we would like to use our regular stochastic gradient optimizers. The KL term in \\(\\mathcal{L}\\) can often be evaluated analytically, especially for the standard choices of distributions. The difficult term is the reconstruction error, which is pesky because it is under expectation of the approximate posterior, which is intractable. However, we can use Monte-Carlo to approximate it:\n\\begin{equation} \\mathbb{E}[\\log p(x|z)] = \\int q(z|x) \\log p(x|z)dz \\approx \\frac{1}{L}\\sum\\limits_{l=1}^L \\log p(x|z^l) \\end{equation}\nwhere we are sampling \\(z^l \\sim q\\). Alas, the problem is that sampling for \\(z\\) means that \\(\\hat{\\mathcal{L}}\\) is not differentiable w.r.t. \\(\\phi\\), so we cannot train the inference network. Another idea might be to directly sample the gradient \\(\\nabla_{\\theta, \\phi}\\mathcal{L}\\). Unfortunately, in 1 it is shown that \\(\\hat{\\nabla}_{\\phi}\\mathcal{L}\\), while unbiased, has very high variance, and training as such tends to diverge more often than not.\nLuckily, some papers from a few years ago (2, 3) fleshed out how we could get around this using an almost embarrasingly simple trick - reparameterization. The idea here is to introduce a random variable \\(\\epsilon\\) that will contain all of the randomness in \\(z\\) for us. We now set up a new variable \\(\\tilde{z}\\) such that:\n\\begin{equation} \\tilde{z} = g_{\\phi}(\\epsilon, x);\\ \\ \\tilde{z} \\sim q(z|x);\\ \\ \\epsilon \\sim p(\\epsilon) \\end{equation}\nso all we need is that \\(g_{\\phi}\\) be differentiable and be able to sample from \\(p_{\\epsilon}\\). This is almost always possible, and in many cases it is even super-easy. For example, if:\n\\begin{equation} q(z|x) = \\mathcal{N}(z; \\mu(x), \\sigma^2(x)) \\end{equation}\nwith \\(\\mu\\) and \\(\\sigma\\) being parameterized by neural networks, then:\n\\begin{equation} p(\\epsilon) = \\mathcal{N}(\\epsilon; 0,1);\\ \\ g(\\epsilon, x) = \\mu(x) + \\epsilon \\otimes \\sigma(x) \\end{equation}\nwhere I am using \\(\\otimes\\) to denote an element-wise multiplication. Reparameterization, simple though it may appear, does two amazing things. (1) It manipulates \\(\\hat{\\mathcal{L}}\\) such that it is differentiable w.r.t. \\(\\phi\\) - we can now jointly train the inference network and the model! (2) It allows us to reduce the variance of the estimator by sharing random numbers across terms and iterations. This may not be the only reason, but regardless, \\(\\nabla_{\\theta,\\phi}\\hat{\\mathcal{L}}\\) exhibits low variance, and in practice converges very nicely. For a more detailed explanation of how and why reparameterization works, I strongly recommend Carl Doersch's excellent tutorial (4).\nStochastic Backpropagation  So that's it. We're ready to put it all together in an algorithm called Auto-encoding Variational Bayes (2), or Stochastic Backpropagation (3) (the thing was concurrently discovered and published by two separate groups). Conveniently, we can run this optimization in mini-batch form, so basically its just stochastic gradient descent with your favorite optimizer on the estimator described above. Just for closure, here is the complete algorithm (in rough pseudo-code):\nfunction stochastic_backprop(data, L): theta, phi \u0026lt;- initialize_variables() repeat: x_batch \u0026lt;- data.next_batch() eps_l \u0026lt;- peps.sample() for l in range(L) gradient \u0026lt;- L(x_batch, eps_l).eval_gradient(theta, phi) theta, phi \u0026lt;- optimizer.update(gradient) until convergence (theta, phi) return theta, phi  Of course, this is very rough psuedo code that is not meant to run. Perhaps in a future post I will go into detail with one model (VAE), and will run through a TensorFlow implementation (including code) with toy data. There I can also discuss how the exact same algorithm can be used to train a BNN, and run through a TF implementation of that with the same data.\nReferences    Paisley, John, Blei, David M, and Jordan, Michael I. Variational Bayesian Inference with Stochastic Search. 2012 \u0026#x21a9;\u0026#xfe0e;\n Kingma, Diederik P and Welling, Max. Auto-encoding variational Bayes. 2013 \u0026#x21a9;\u0026#xfe0e;\n Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra, Daan. Stochastic backpropagation and approximate inference in deep generative models. 2014 \u0026#x21a9;\u0026#xfe0e;\n Doerch, Carl. Tutorial on Variational Autoencoders. 2016 \u0026#x21a9;\u0026#xfe0e;\n   ","date":1500854400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1500854400,"objectID":"58d40f1632b1b259eec421c59005ff69","permalink":"/post/stochastic-bp/","publishdate":"2017-07-24T00:00:00Z","relpermalink":"/post/stochastic-bp/","section":"post","summary":"Recently I've had to train a few deep generative models with stochastic backpropagation. I've been working with variational autoencoders and Bayesian neural networks. If you've read the literature on these training procedures and models, you probably found the theory quite complete. When I implemented these models however, I found that quite a lot of elbow grease is required to get them to work well.\nFrom talking to some other researchers here, it seems like people dealing with these models are thirsty for practical advice.","tags":null,"title":"Training Deep Models with Stochastic Backpropagation","type":"post"}]